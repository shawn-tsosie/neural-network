STEPS OF NEURAL NETWORK
1. Pre-process
		a. Normalize Training Set
		b. Shuffle Training Set
2. Train Neural Network
		a. Initialize Weights (Keep track of them in a list)
		b. Begin mini-batch ADAM for some number of epochs
			 Do the following until all elements of the training set are used.
				i. 		Feed-forward (Keep track of Loss)
				ii. 	Back-Prop	
				iii.	Update Weights via ADAM algorithm.
3. Evaluate Test Loss via predict function

def conv2dlayer(X, step_size=1, filter_size=3):
	conv_array = np.empty_like(X)
	padded = pad(X, step_size)
	filter = create_filter(filter_size, filter_size)
	for i in range(X.shape[1]):
		for j in range(X.shape[2]):
			conv_array[i, j] = np.dot(padded[i:3+i, j:3+j], filter)
	return conv_array 

def conv2dgrad(X, filter):
	


# X can be any sized ndarray, e.g. a (3, 2, 2, 5) ndarray will become
# pad six 2 x 5 matrices.
def pad(X, pad_size=1):
	pad_array = np.empty_like(X)
	pad_array = X.copy()

	# Initializing the padding matrix
	zero_col_shape = ()
	for i in range(len(X.shape) - 1):
		zero_col_shape = zero_col_shape + (X.shape[i], )
	zero_col_shape = zero_col_shape + (1,) 
	zero_col = np.zeros(zero_col_shape)
	
	# Left and right append
	for i in range(pad_size):
		pad_array = np.c_[zero_col, pad_array, zero_col]
	
	ncols = pad_array.shape[1]
	zero_row = np.zeros((1, ncols))

	# Top and bottom append
	for i in range(pad_size):	
		pad_array = np.r_[zero_row, pad_array, zero_row]
	return pad_array

def create_filter(filter_size1=3, filter_size2=3):
	return np.random.randn(filter_size1, filter_size2) 
